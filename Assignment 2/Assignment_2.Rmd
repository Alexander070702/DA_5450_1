---
title: 'Data Analytics: Assignment 2 Group 6'
output: html_document
date: "29.03.2023"
---

### Load the caret package for data partitioning

```{r}
library("caret")
```

### Task 1: Load the data

Load the churn dataset from a [URL](https://statmath.wu.ac.at/~malsiner/datasets/churn.csv) and convert the churn variable to binary

```{r}
churndat <- read.csv(url("https://statmath.wu.ac.at/~malsiner/datasets/churn.csv"))
churndat$churn <- ifelse(churndat$churn == "Yes", 1, 0)
```

Convert the internationalplan and voicemailplan variables to binary

```{r}
churndat$internationalplan <- ifelse(churndat$internationalplan == "yes", 1, 0)
churndat$voicemailplan <- ifelse(churndat$voicemailplan == "yes", 1, 0)
```
```{r}
#structure of the data
str(churndat)

#Generate appropriate graphs for all variables differentiated into churn vs. no churn.
#accountlength
ggplot(churndat, aes(x = accountlength, fill = factor(churn))) + geom_histogram(bins = 20, position = "identity") + labs(title = "Account Length", x = "Account Length", y = "Frequency")
#internationalplan
ggplot(churndat, aes(x = internationalplan, fill = factor(churn))) + geom_bar(position = "identity") + labs(title = "International Plan", x = "International Plan", y = "Frequency")
#voicemailplan
ggplot(churndat, aes(x = voicemailplan, fill = factor(churn))) + geom_bar(position = "identity") + labs(title = "Voicemail Plan", x = "Voicemail Plan", y = "Frequency")
#numbervmailmessages
ggplot(churndat, aes(x = numbervmailmessages, fill = factor(churn))) + geom_histogram(bins = 20, position = "identity") + labs(title = "Number of Voicemail Messages", x = "Number of Voicemail Messages", y = "Frequency")
#totaldayminutes
ggplot(churndat, aes(x = totaldayminutes, fill = factor(churn))) + geom_histogram(bins = 20, position = "identity") + labs(title = "Total Day Minutes", x = "Total Day Minutes", y = "Frequency")
#totaleveminutes
ggplot(churndat, aes(x = totaleveminutes, fill = factor(churn))) + geom_histogram(bins = 20, position = "identity") + labs(title = "Total Eve Minutes", x = "Total Eve Minutes", y = "Frequency")
#totalnightminutes
ggplot(churndat, aes(x = totalnightminutes, fill = factor(churn))) + geom_histogram(bins = 20, position = "identity") + labs(title = "Total Night Minutes", x = "Total Night Minutes", y = "Frequency")
#totalintlcalls
ggplot(churndat, aes(x = totalintlcalls, fill = factor(churn))) + geom_histogram(bins = 20, position = "identity") + labs(title = "Total International Calls", x = "Total International Calls", y = "Frequency")
#totalintlcharge
ggplot(churndat, aes(x = totalintlcharge, fill = factor(churn))) + geom_histogram(bins = 20, position = "identity") + labs(title = "Total International Charge", x = "Total International Charge", y = "Frequency")
#numbercustomerservicecalls
ggplot(churndat, aes(x = numbercustomerservicecalls, fill = factor(churn))) + geom_histogram(bins = 20, position = "identity") + labs(title = "Number of Customer Service Calls", x = "Number of Customer Service Calls", y = "Frequency")

```

There seems to be a relationship with churn and international plan. If the user has no International plan the churn rate is way lower compared to users with an international plan
It is the opposite with the Voicemail plan, if the user has one the churn rate is lower.
Also with a rising number of customer service calls the churn rate seems to go up. The number of voicemail messages also seems to have some sort of influence, users with 0 seem to have a higher churn rate.

For all the other variables it is difficult to tell and there seems to be no real correlation at first sight. 


### Task 2: Splitting the data to training set and test set

Split the data into training and testing sets, with 80% for training and 20% for testing

```{r}
set.seed(123) # for reproducibility
train_index <- createDataPartition(churndat$churn, p = 0.8, list = FALSE)
train <- churndat[train_index, ]
test <- churndat[-train_index, ]
```

### Task 3: Full logistic regression model

Fit a logistic regression model to the training data using all variables as predictors

```{r}
mlm <- glm(churn ~ ., data = train, family = binomial())
```

Print a summary of the logistic regression model

```{r}
summary(mlm)
```

Print a summary of the logistic regression model

```{r}
summary(mlm)
```

The significant variables in this model are: <b>internationalplan</b>, <b>voicemailplan</b>, <b>numbervmailmessages</b>, <b>totalintlcalls</b> and <b>numbercustomerservicecalls</b>.

### Task 4: Logistic model stepwise analysis

Perform stepwise variable selection on the logistic regression model using both forward and backward selection The 'trace' argument controls the amount of output produced during the stepwise procedure

```{r}
mlm_step <- stepAIC(mlm, direction = "both", trace = FALSE)
```

Print a summary of the stepwise logistic regression model

```{r}
summary(mlm_step)
```

Variables in the final model are : accountlength, internationalplan, voicemailplan, numbervmailmessages, totaldayminutes, totaleveminutes, totalnightminutes, totalintlcalls, totalintlcharge and numbercustomerservicecalls.

The coefficient of totaldayminutes suggests that there is a positive correlation between minutes talked and probability of leaving the company. Similarly, the coefficient of voicemailplan suggests a positive correlation between having a plan including a voicemail and the probability of leaving the company.

Calculate the predicted probabilities and predicted classes of the testing data using the stepwise logistic regression model

```{r}
test$prob_churn_step <- predict(mlm_step, newdata = test, type = "response")
test$class_churn_step <- ifelse(test$prob_churn_step > 0.5, 1, 0)
```
```{r}
# Calculate the predicted probabilities and predicted classes of the testing data using the stepwise logistic regression model
test$prob_churn_step <- predict(mlm_step, newdata = test, type = "response")
test$class_churn_step <- ifelse(test$prob_churn_step > 0.5, 1, 0)

# Calculate the confusion matrix and extract the true positive, false positive, true negative, and false negative counts
confusion <- table(test$churn, test$class_churn_step)
tp <- confusion[2,2]
fp <- confusion[1,2]
tn <- confusion[1,1]
fn <- confusion[2,1]

# Calculate the accuracy, recall, and precision of the stepwise logistic regression model on the testing data
accuracy_step <- sum(diag(confusion)) / sum(confusion)
recall_step <- tp / (tp + fn)
precision_step <- tp / (tp + fp)

# Print the accuracy, recall, and precision of the stepwise logistic regression model on the testing data
cat("Accuracy: ", round(accuracy_step, 4), "\n")
cat("Recall: ", round(recall_step, 4), "\n")
cat("Precision: ", round(precision_step, 4), "\n")
```

Print the accuracy, sensitivity, and specificity of the stepwise logistic regression model on the testing data

```{r}
cat("Accuracy: ", round(accuracy_step, 4), "\n")
cat("Sensitivity: ", round(sensitivity_step, 4), "\n")
cat("Specificity: ", round(specificity_step, 4), "\n")
```
### Task 5: k-NN modell using 5 fold cross-validation

```{r}
fitControl <- trainControl(method = "cv", number = 5)

logFit <- train(as.factor(churn) ~ scale(accountlength) + internationalplan + voicemailplan + scale(totalintlcharge) + scale(numbervmailmessages) + scale(totaldayminutes) + scale(totaldaycharge) + scale(totalevecalls) + scale(totaleveminutes) + scale(totalnightminutes) + scale(totalintlminutes) + 
scale(totalintlcalls) + scale(numbercustomerservicecalls) + scale(totalnightcharge) + scale(totaldaycalls) + scale(totalevecharge) + scale(totalnightcalls),data = train, method= "knn", tuneGrid = data.frame(k = 1:10), trControl = fitControl)

logFit

```

### Task 6: Naive Bayes model estemated on training sample

```{r}
library(e1071)
fit_nb = naiveBayes(churn ~ internationalplan + voicemailplan + numbercustomerservicecalls + totalintlcalls + numbervmailmessages, data = train)

fit_nb
```
### Task 7: Comparing the different methods

```{r}
# k-NN model
knn_pred <- predict(logFit, newdata = test)

# Naive Bayes model
nb_pred <- predict(fit_nb, newdata = test)

# Confusion matrix for stepwise logistic regression model
library(caret)
conf_mat_lr <- confusionMatrix(as.factor(test$class_churn_step), as.factor(test$churn))
acc_lr <- conf_mat_lr$overall['Accuracy']
rec_lr <- conf_mat_lr$byClass['Sensitivity']
prec_lr <- conf_mat_lr$byClass['Pos Pred Value']

# Confusion matrix for k-NN model
conf_mat_knn <- confusionMatrix(as.factor(knn_pred), as.factor(test$churn))
acc_knn <- conf_mat_knn$overall['Accuracy']
rec_knn <- conf_mat_knn$byClass['Sensitivity']
prec_knn <- conf_mat_knn$byClass['Pos Pred Value']

# Confusion matrix for Naive Bayes model
conf_mat_nb <- confusionMatrix(as.factor(nb_pred), as.factor(test$churn))
acc_nb <- conf_mat_nb$overall['Accuracy']
rec_nb <- conf_mat_nb$byClass['Sensitivity']
prec_nb <- conf_mat_nb$byClass['Pos Pred Value']

# We can now print out the results:
cat("Accuracy of stepwise logistic regression model: ", round(acc_lr, 4), "\n")
cat("Recall of stepwise logistic regression model: ", round(rec_lr, 4), "\n")
cat("Precision of stepwise logistic regression model: ", round(prec_lr, 4), "\n\n")

cat("Accuracy of k-NN model: ", round(acc_knn, 4), "\n")
cat("Recall of k-NN model: ", round(rec_knn, 4), "\n")
cat("Precision of k-NN model: ", round(prec_knn, 4), "\n\n")

cat("Accuracy of Naive Bayes model: ", round(acc_nb, 4), "\n")
cat("Recall of Naive Bayes model: ", round(rec_nb, 4), "\n")
cat("Precision of Naive Bayes model: ", round(prec_nb, 4), "\n")
```
Based on the results, we can see that the stepwise logistic regression model and the Naive Bayes model have similar accuracy, recall, and precision, while the k-NN model has lower accuracy and recall but higher precision. It's worth noting that the choice of the best model ultimately depends on the specific requirements and objectives of the classification problem.
